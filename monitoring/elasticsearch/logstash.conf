# Logstash Configuration for PelotonIQ Log Processing
input {
  # Kubernetes pod logs via Filebeat
  beats {
    port => 5044
  }
  
  # Direct syslog input
  syslog {
    port => 5514
  }
  
  # HTTP input for application logs
  http {
    port => 8080
    codec => json
  }
  
  # Kafka input for high-volume logs
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["application-logs", "audit-logs", "metrics"]
    codec => json
    group_id => "logstash-consumers"
  }
}

filter {
  # Add timestamp if missing
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
  
  # Parse Kubernetes metadata
  if [kubernetes] {
    mutate {
      add_field => {
        "k8s_namespace" => "%{[kubernetes][namespace_name]}"
        "k8s_pod" => "%{[kubernetes][pod_name]}"
        "k8s_container" => "%{[kubernetes][container_name]}"
        "k8s_node" => "%{[kubernetes][node_name]}"
      }
    }
  }
  
  # Parse application logs
  if [fields][service] == "pelotoniq-backend" {
    # Parse Spring Boot logs
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} \[%{DATA:thread}\] %{DATA:logger} : %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse Spring Boot actuator metrics
    if [log_message] =~ /actuator/ {
      mutate {
        add_tag => ["actuator", "metrics"]
        add_field => { "log_type" => "metrics" }
      }
    }
    
    # Parse database queries
    if [log_message] =~ /Hibernate:|SQL:/ {
      mutate {
        add_tag => ["database", "sql"]
        add_field => { "log_type" => "database" }
      }
    }
    
    # Parse security events
    if [log_message] =~ /Security|Authentication|Authorization/ {
      mutate {
        add_tag => ["security", "audit"]
        add_field => { "log_type" => "security" }
      }
    }
  }
  
  # Parse Node.js data processor logs
  if [fields][service] == "pelotoniq-data-processor" {
    # Parse JSON structured logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
    } else {
      # Parse text logs
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}"
        }
      }
    }
    
    # Tag data processing events
    if [log_message] =~ /processing|scraping|queue/ {
      mutate {
        add_tag => ["data-processing", "etl"]
        add_field => { "log_type" => "data_processing" }
      }
    }
  }
  
  # Parse AI services logs
  if [fields][service] == "pelotoniq-ai-services" {
    # Parse Python logs
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:log_timestamp} - %{DATA:module} - %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}"
      }
    }
    
    # Tag ML/AI events
    if [log_message] =~ /model|inference|training|prediction/ {
      mutate {
        add_tag => ["machine-learning", "ai"]
        add_field => { "log_type" => "ml" }
      }
    }
    
    # Parse TensorFlow logs
    if [log_message] =~ /tensorflow|tf\.|keras/ {
      mutate {
        add_tag => ["tensorflow", "deep-learning"]
        add_field => { "framework" => "tensorflow" }
      }
    }
  }
  
  # Parse error patterns
  if [log_level] in ["ERROR", "FATAL", "error", "fatal"] {
    mutate {
      add_tag => ["error"]
      add_field => { "alert_level" => "high" }
    }
    
    # Extract stack traces
    if [log_message] =~ /Exception|Error|Traceback/ {
      mutate {
        add_tag => ["exception", "stacktrace"]
      }
    }
  }
  
  # Parse performance metrics
  if [log_message] =~ /duration|latency|response_time|processing_time/ {
    grok {
      match => {
        "log_message" => ".*duration:?%{NUMBER:duration:float}.*"
      }
    }
    
    if [duration] {
      mutate {
        add_tag => ["performance", "metrics"]
        add_field => { "metric_type" => "duration" }
      }
    }
  }
  
  # Parse business events
  if [log_message] =~ /user_registration|team_creation|race_analysis|prediction/ {
    mutate {
      add_tag => ["business", "events"]
      add_field => { "log_type" => "business" }
    }
  }
  
  # Geo IP enrichment for external requests
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Add environment information
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "service_version" => "${SERVICE_VERSION:unknown}"
      "cluster" => "${CLUSTER_NAME:pelotoniq-prod}"
    }
  }
  
  # Remove unwanted fields
  mutate {
    remove_field => ["agent", "ecs", "input", "host"]
  }
  
  # Convert log level to lowercase for consistency
  if [log_level] {
    mutate {
      lowercase => ["log_level"]
    }
  }
  
  # Parse log timestamp
  if [log_timestamp] {
    date {
      match => [ "log_timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
}

output {
  # Main application logs to Elasticsearch
  if "application" in [tags] or [fields][service] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-application-logs-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-application"
      template => "/usr/share/logstash/templates/pelotoniq-application.json"
      template_overwrite => true
    }
  }
  
  # Infrastructure logs
  if "infrastructure" in [tags] or [log_type] == "infrastructure" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-infrastructure-logs-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-infrastructure"
      template => "/usr/share/logstash/templates/pelotoniq-infrastructure.json"
      template_overwrite => true
    }
  }
  
  # Security and audit logs
  if "security" in [tags] or "audit" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-security-logs-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-security"
      template => "/usr/share/logstash/templates/pelotoniq-security.json"
      template_overwrite => true
    }
  }
  
  # Performance metrics
  if "performance" in [tags] or "metrics" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-performance-logs-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-performance"
      template => "/usr/share/logstash/templates/pelotoniq-performance.json"
      template_overwrite => true
    }
  }
  
  # Business events
  if "business" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-business-events-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-business"
      template => "/usr/share/logstash/templates/pelotoniq-business.json"
      template_overwrite => true
    }
  }
  
  # Error logs to dedicated index
  if "error" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-errors-%{+YYYY.MM.dd}"
      template_name => "pelotoniq-errors"
      template => "/usr/share/logstash/templates/pelotoniq-errors.json"
      template_overwrite => true
    }
    
    # Send critical errors to alerting system
    if [alert_level] == "high" {
      http {
        url => "${ALERT_WEBHOOK_URL}"
        http_method => "post"
        content_type => "application/json"
        format => "json"
        mapping => {
          "service" => "%{[fields][service]}"
          "level" => "%{log_level}"
          "message" => "%{log_message}"
          "timestamp" => "%{@timestamp}"
          "pod" => "%{k8s_pod}"
          "namespace" => "%{k8s_namespace}"
        }
      }
    }
  }
  
  # Dead letter queue for unparseable logs
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "pelotoniq-unparsed-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (only in development)
  if "${ENVIRONMENT:production}" == "development" {
    stdout {
      codec => rubydebug
    }
  }
}